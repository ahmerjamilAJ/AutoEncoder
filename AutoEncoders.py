# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Autoencoders

"""

# part 1 and 2 weight files are ...
"/content/drive/MyDrive/dl/part1.pt"
"/content/drive/MyDrive/dl/part2.pt"

import glob
import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from torchsummary import summary
import random
from google.colab.patches import cv2_imshow
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from torchvision import models
from tqdm import tqdm

from google.colab import drive
drive.mount('/content/gdrive')

"""### Part 1: Image Completion

Image Completion is the task of filling missing parts of a given image with the help of information from the known parts of the image. This is an application that takes an image with a missing part as input and gives a completed image as the result.

We will be using Autoencoder to do this task. We will train our network on Images with missing parts passed with true images so that autoencoder generate an image with the missing parts.
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Data

The data we are using is [Flickr-Faces-HQ Dataset (FFHQ)](https://github.com/NVlabs/ffhq-dataset). It is an unlabelled dataset used for training GANs and other image generation algorithms. The original dataset has images of size 1024 by 1024 but we have only taken 128 by 128 images. Please download the dataset from this link (https://drive.google.com/drive/folders/1tg-Ur7d4vk1T8Bn0pPpUSQPxlPGBlGfv?usp=sharing)

Since the dataset is large (contains 70,000 image), you'll have to write a DataLoader that loads the data. For this you'll be using the Dataset and DataLoader class that are available in pytorch. For reference, you can get help from this link (https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)

The input to the network will consist of a image that is modified as shown below. The output label will be the original image.

![title](images/face_dataset.png)
"""

class FaceData(Dataset):
    def __init__(self,files): # add additional parameters needed to load the dataset e.g dataset path
        self.files = files
    def __len__(self):
        return len(self.files)
    def __getitem__(self, idx):
        file = self.files[idx] 
        image = np.transpose(cv2.cvtColor(cv2.rectangle(cv2.imread(file),(random.randint(10,100),random.randint(10,100)),(random.randint(10,100)+20,random.randint(10,100)+10),(255,255,255),-1), cv2.COLOR_BGR2RGB))
        image2 = np.transpose(cv2.cvtColor( cv2.imread(file), cv2.COLOR_BGR2RGB))
        return image/255,image2/255

import torch.nn.functional as F

"""autoencoder model here. (https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html).
"""

class ImageCompletionNet(nn.Module):
    def __init__(self):
      super().__init__()
      self.conv1 =  nn.Conv2d(3,16,kernel_size= 2,stride= 2)
      self.batch1=  nn.BatchNorm2d(16)
      self.conv2 =  nn.Conv2d(16,32,kernel_size= 2,stride= 2)
      self.batch2=  nn.BatchNorm2d(32)
      self.conv3=   nn.Conv2d(32,64,kernel_size= 2,stride= 2)
      self.batch3=   nn.BatchNorm2d(64)
      self.conv4=   nn.Conv2d(64,64,kernel_size= 2,stride= 2)
      self.batch4=   nn.BatchNorm2d(64)
      self.conv5=   nn.Conv2d(64,64,kernel_size= 2,stride= 2)
      self.batch5 =   nn.BatchNorm2d(64)

          
      

      
      self.convt1=     nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)
      self.batch6=     nn.BatchNorm2d(64)
      self.convt2=    nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)
      self.batch7=     nn.BatchNorm2d(64)
      self.convt3=       nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)
      self.batch8=     nn.BatchNorm2d(32)
      self.convt4=       nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)
      self.batch9=     nn.BatchNorm2d(16)
      self.convt5=       nn.ConvTranspose2d(16, 3, kernel_size=2, stride=2)
      self.sig=     nn.Sigmoid()
      
    def forward(self,x):
      x =  self.batch1(self.conv1(x))
      x = self.batch2(self.conv2(x))
      x = self.batch3(self.conv3(x))
      x = self.batch4(self.conv4(x))
      x = self.batch5(self.conv5(x))
      
      x = self.batch6(self.convt1(x))
      x = self.batch7(self.convt2(x))
      x = self.batch8(self.convt3(x))
      x = self.batch9(self.convt4(x))
      x = self.sig(self.convt5(x))
      return x

model = ImageCompletionNet()
print(model)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device)
summary(model,(3,128,128))

from google.colab import drive
drive.mount('/content/drive')

"""Load the data using Custom Dataset class defined above and load your DataLoader. Divide the whole dataset into train, validation and test set.

"""

import random
l = glob.glob('/content/gdrive/MyDrive/thumbnails128x128/*/*') 
lengthof = len(l)
lengthof = (lengthof*0.2)
lengthof = int(lengthof)
NewL = l[:lengthof] 
random.shuffle(NewL)
transform = transforms.Compose([transforms.ToTensor()])


train_loader = DataLoader(dataset= FaceData(NewL[:int(len(NewL)*0.8)]),batch_size=32,shuffle = True)
valid_loader = DataLoader(dataset=FaceData(NewL[-int(len(NewL)*0.1):]),batch_size=1,shuffle = True)
test_loader = DataLoader(dataset= FaceData(NewL[-int(len(NewL)*0.1):]),batch_size=1,shuffle = True)

for x in train_loader:
  cv2_imshow(cv2.cvtColor((255*np.transpose(x[0][0].detach().cpu().numpy())).astype(np.uint8), cv2.COLOR_BGR2RGB))
  cv2_imshow(cv2.cvtColor((255*np.transpose(x[1][0].detach().cpu().numpy())).astype(np.uint8), cv2.COLOR_BGR2RGB))    
  break

"""training pipeline and evaluation using the validation loss"""

epochs = 15
loss_function = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(),
                             lr = 0.03)

losses = []
for epoch in tqdm(range(epochs)):
  train_loss = 0
  for data in tqdm(train_loader):
    input = data[0].to(device, dtype=torch.float)
    target = data[1].to(device, dtype=torch.float)
    optimizer.zero_grad()
    reconstructed = model(input)
    loss = loss_function(reconstructed, target)
    loss.backward()
    optimizer.step()
    train_loss = train_loss + loss.item()
  
  losses.append(train_loss/1000)
  print("Epoch # ", epoch," has training loss = ",train_loss)

torch.save(model.state_dict(), '/content/gdrive/MyDrive/dl/part1.pt')

count = 0
for i in test_loader:
  count=count+1
  target = i[1].to(device, dtype=torch.float)
  reconstructed = model(i[0].to(device, dtype=torch.float))
  one = (255*np.transpose(reconstructed[0].detach().cpu().numpy())).astype(np.uint8), cv2.COLOR_BGR2RGB
  two = (255*np.transpose(target[0].detach().cpu().numpy())).astype(np.uint8), cv2.COLOR_BGR2RGB
  cv2_imshow(cv2.cvtColor(one))
  cv2_imshow(cv2.cvtColor(two) ) 
  if count ==1: 
    break

"""
# Part 2: Image Denoising

An application of autoencoders includes image denoising. For this part we will be using the person dataset that is a subset of the COCO dataset. (https://drive.google.com/file/d/1gHTLS6dvEp31X2dWBdLrvGr--5WOm1Z3/view?usp=sharing).
NOTE: This part is important and part 3 of the assignment uses the model that you'll build for denoising.
"""

!unzip '/content/gdrive/MyDrive/person_dataset.zip'

class ImageDenoising(Dataset):
    def __init__(self,files):
        self.files = files

    def __len__(self):
        return len(self.files) # this would return us the number of images we have

    def add_noise(self,img):
      row = img.shape[0]
      col = img.shape[1]

      for i in range(random.randint(1000, 3000)):
          img[random.randint(0, img.shape[0] - 1)][random.randint(0,  img.shape[1] - 1)] = 255
        
      for i in range(random.randint(1000 , 3000)):
          img[random.randint(0, row - 1)][random.randint(0, col - 1)] = 0
      return img

    def __getitem__(self, idx):       
        file = self.files[idx] 
        image = np.transpose(cv2.cvtColor(self.add_noise(cv2.resize(cv2.imread(file),dsize= (256,256))), cv2.COLOR_BGR2RGB))
        image2 = np.transpose(cv2.cvtColor(cv2.resize( cv2.imread(file),dsize= (256,256)), cv2.COLOR_BGR2RGB))
        return image/255, image2/255

import random
l = glob.glob('/content/person_dataset/images/*')
lengthof = len(l)
lengthof = (lengthof*0.2)
lengthof = int(lengthof)
lengthNew = l[:lengthof] 
random.shuffle(lengthNew)
train_loader2 = DataLoader(dataset=ImageDenoising(lengthNew[:int(len(lengthNew)*0.8)]),batch_size=64,shuffle = True)
valid_loader2 = DataLoader(dataset=ImageDenoising(lengthNew[-int(len(lengthNew)*0.1):]),batch_size=1,shuffle = True)
test_loader2 = DataLoader(dataset=ImageDenoising(lengthNew[-int(len(lengthNew)*0.1):]),batch_size=1,shuffle = True)

for i in test_loader2:
  input , target = i

  one = (255*np.transpose(input[0].detach().cpu().numpy())).astype(np.uint8), cv2.COLOR_BGR2RGB
  two = (255*np.transpose(target[0].detach().cpu().numpy())).astype(np.uint8), cv2.COLOR_BGR2RGB
  cv2_imshow(cv2.cvtColor(one))
  cv2_imshow(cv2.cvtColor(two))
  break


import torch.nn.functional as F

class Denoising(nn.Module):
    def __init__(self):
      super().__init__()
      
          
      self.conv1=    nn.Conv2d(3,16,kernel_size= 2 , stride = 2)
      self.batch1=   nn.BatchNorm2d(16)
      self.conv2=    nn.Conv2d(16,32,kernel_size= 4)
      self.batch2=   nn.BatchNorm2d(32)
      self.conv3=    nn.Conv2d(32,64,kernel_size= 4)
      self.relu =    nn.ReLU()
      self.batch3=   nn.BatchNorm2d(64)
      self.conv4=    nn.Conv2d(64,64,kernel_size= 2,stride = 2)   
      self.batch4=   nn.BatchNorm2d(64)          
      self.convt1 =  nn.ConvTranspose2d(64, 64, kernel_size=2,stride = 2)
      self.batch5 =       nn.BatchNorm2d(64)      
      self.convt2=      nn.ConvTranspose2d(64, 32, kernel_size=4)
      self.batch6=      nn.BatchNorm2d(32)
      self.convt3=      nn.ConvTranspose2d(32, 16, kernel_size=4)
      self.batch7=       nn.BatchNorm2d(16)
      self.convt4=      nn.ConvTranspose2d(16, 3, kernel_size=2, stride=2)
      self.batch8=       nn.BatchNorm2d(3)
      self.sig=     nn.Sigmoid()
      


    def forward(self,x):
      x =  self.batch1(self.conv1(x))
      x =  self.batch2(self.conv2(x))
      x =  self.batch3(self.relu(self.conv3(x)))
      x =  self.batch4(self.conv4(x))
      x = self.batch5(self.convt1(x))
      x = self.batch6(self.relu(self.convt2(x)))
      x = self.batch7(self.convt3(x))
      x = self.sig(self.batch8(self.convt4(x)))

      return x


model = Denoising()
print(model)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device)
summary(model,(3,128,128))

losses = []
epochs = 10
loss_function = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(),
                             lr = 0.03)
for epoch in tqdm(range(epochs)):

  train_loss = 0
  print(" ")
  for data in tqdm(train_loader2):
    input = data[0].to(device, dtype=torch.float)
    target = data[1].to(device, dtype=torch.float)
    optimizer.zero_grad()
    reconstructed = model(input)
    loss = loss_function(reconstructed, target)
    loss.backward()
    optimizer.step()
    train_loss = train_loss+ loss.item()
  print(" ")
  losses.append(train_loss)
  print("Epoch # ",epoch,"training loss = ",train_loss)

torch.save(model.state_dict(), '/content/gdrive/MyDrive/dl/part2.pt')

#  testing pipeline 
model = Denoising()
model.load_state_dict(torch.load('/content/gdrive/MyDrive/dl/part2.pt'))
model = model.to(device)
model.eval()


count = 0
for value in test_loader2:
  inp = value[0].to(device, dtype=torch.float)
  tar = value[1].to(device, dtype=torch.float)
  reconstructed = model(inp)
  one = (255*np.transpose(inp[0].detach().cpu().numpy())).astype(np.uint8), cv2.COLOR_BGR2RGB
  two =(255*np.transpose(reconstructed[0].detach().cpu().numpy())).astype(np.uint8), cv2.COLOR_BGR2RGB
  three =(255*np.transpose(tar[0].detach().cpu().numpy())).astype(np.uint8), cv2.COLOR_BGR2RGB
  cv2_imshow(cv2.cvtColor(one))
  cv2_imshow(cv2.cvtColor(two))
  cv2_imshow(cv2.cvtColor(three))
  if count == 5:
    break
  count += 1

